{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ikramettiache/Scraping-site-lemonde/blob/main/Scraping_des_donn%C3%A9es_Off.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YJO76eqbwM",
        "outputId": "c3646545-6478-4e10-cb3d-1be007515e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install bs4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W44FrfMl7pg",
        "outputId": "3984ca27-7ee0-48a3-ab98-528606975441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install dateparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging"
      ],
      "metadata": {
        "id": "ax-nlarxYav6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvs8nNmejaYA"
      },
      "outputs": [],
      "source": [
        "#Importer les packages nécessaires\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "from pandas.core.frame import DataFrame\n",
        "import dateparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bBKZLKyAh3F"
      },
      "outputs": [],
      "source": [
        "# récupérer le code HTML sous format text\n",
        "def get_text(s):\n",
        "   url=\"https://www.lemonde.fr/recherche/?search_keywords=crise&start_at=08/01/2022&end_at=08/02/2022&search_sort=date_desc&page=\"+str(s)\n",
        "   rep = requests.get(url)\n",
        "   if rep.ok:\n",
        "     soup=BeautifulSoup(rep.text)\n",
        "   else :\n",
        "    print(\" Erreur Request get\")\n",
        "  # if rep not ok ==> rep.text = ?  \n",
        "   return rep.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7HoulJPCXLl"
      },
      "outputs": [],
      "source": [
        "#Récupérer temps de lecture, catégorie, date de publication\n",
        "def get_read_time_categorie(url2):\n",
        "    catégorie=''\n",
        "    date_de_publication=''\n",
        "    temps_de_lecture=''\n",
        "    rep = requests.get(url2)\n",
        "    soup2=BeautifulSoup(rep.text)\n",
        "    p=soup2.find('p',{'class':\"meta__reading-time\"})\n",
        "    s=soup2.find('span',{'class':\"meta__reading-time\"})\n",
        "    if p:\n",
        "      temps_de_lecture=(re.sub(\"temps de lecture[-_/./,]*\",\"\",p.get_text().lower()))\n",
        "    if s:\n",
        "      # if p and s,first temps_de_lecture will  be overridden \n",
        "      temps_de_lecture=(re.sub(\"temps de lecture[-_/./,]*\",\"\",s.get_text().lower()))\n",
        "    sections=soup2.findAll('section', {'class': True})\n",
        "    for section in sections:\n",
        "      if('zone' or 'article' or ('meta' and 'date') in section['class']):\n",
        "        aa=section.findAll('a',{'class': True})\n",
        "        span_date=section.findAll('span',{'class': 'meta__date'})\n",
        "        for a in aa :\n",
        "            if((\"logo\" or \"article\") in a['class'][0]):\n",
        "              catégorie=a.text\n",
        "              break\n",
        "        for span in span_date:\n",
        "           date_de_publication=dateparser.parse(re.sub(\"publié|le|à|[-_/./,]*|mis(e)* à* jour.*\",'',(span.text).lower()))\n",
        "      return catégorie,date_de_publication,temps_de_lecture\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Récupérer title2 depuis URL de l'article\n",
        "def get_title_2(url):\n",
        "  title2=''\n",
        " # p='-*[A-ZA-z0-9]+-[A-Za-z0-9]+'\n",
        "  pattern='(/article/)([0-9]+/)+(-*[A-ZA-z0-9]+-[A-Za-z0-9]+)+_'\n",
        "  m=re.search(pattern,url)\n",
        "  if m:\n",
        "    title2=re.sub('(/article/)([0-9]+/)+|-|_',' ',m.group())\n",
        "    # return re.sub('(/article/)([0-9]+/)+|-|_',' ',m.group()).strip() -- bla matzidi title2\n",
        "  return title2.strip()"
      ],
      "metadata": {
        "id": "XF-4Z6lTZQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7wVYxf96g0n"
      },
      "outputs": [],
      "source": [
        "#Récupérer title1, lien de l'article ainsi que les autres attributs et  les stocker dans une dataframe\n",
        "def get_all_line_dataframe(rep,df):\n",
        "  title=''\n",
        "  categorie=date_de_publication=read_time=''\n",
        "  soup=BeautifulSoup(rep)\n",
        "  sections=soup.findAll('section')\n",
        "  for section in sections:\n",
        "    if(section['class'][0]==\"teaser\"):\n",
        "        aa=section.findAll('a')\n",
        "        titles=section.findAll('h3')\n",
        "        for a in aa:\n",
        "          urla=a['href']\n",
        "          if '/article/'in a['href']:\n",
        "            categorie,date_de_publication,read_time=get_read_time_categorie(urla)\n",
        "            # else blach makadir walo\n",
        "          else :\n",
        "            continue\n",
        "        for title in titles:\n",
        "          title=title.text\n",
        "    if categorie.strip()=='Économie'and not (pd.Series([title]).isin(df.title)[0]):\n",
        "     title2=get_title_2(a['href'])\n",
        "     df=df.append(\n",
        "    {\"title\": title,\n",
        "     \"title2\":title2,\n",
        "     \"categorie\": categorie,\n",
        "     \"date\":date_de_publication,\n",
        "     \"read\":read_time,\n",
        "     \"lien\":a['href']\n",
        "     }, ignore_index=True)\n",
        "        # had condition zidiha m3a if lawla and w safi  \n",
        "     if(len(df)>=2):\n",
        "       break\n",
        "  return df      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSmYRSySt1wK"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  df = pd.DataFrame(columns=['title','categorie','date','read','lien','title2'])\n",
        "  s=1\n",
        "  # hadi diriha \n",
        "  # for index in range(len(df)):\n",
        "  #    df=get_all_line_dataframe(get_text(index),df)\n",
        "  while len(df)<5:\n",
        "    rep=get_text(s)\n",
        "    df=get_all_line_dataframe(rep,df)\n",
        "    s=s+1\n",
        "    sh=logging.debug(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['title','categorie','date','read','lien','title2'])\n",
        "s=1\n",
        "  # hadi diriha \n",
        "  # for index in range(len(df)):\n",
        "  #    df=get_all_line_dataframe(get_text(index),df)\n",
        "while len(df)<2: \n",
        "    df=get_all_line_dataframe(get_text(s),df)\n",
        "s=s+1\n",
        "sh=logging.debug(df)"
      ],
      "metadata": {
        "id": "sNPnsRV9gYSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.info(sh)\n",
        "logging.debug(df)"
      ],
      "metadata": {
        "id": "uw7qVkzHgJAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enregistrer dans un csv et créer dossier si not exist\n",
        "import os\n",
        "\n",
        "outname = 'myData.csv'\n",
        "\n",
        "outdir = r'C:\\file_csv'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "with open(fullname, 'a') as f:\n",
        "    df.to_csv(f, header=f.tell()==0,index=False)"
      ],
      "metadata": {
        "id": "1sxxKpxqUEmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Scraping des données Off",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}