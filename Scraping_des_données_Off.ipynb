{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ikramettiache/Scraping-site-lemonde/blob/main/Scraping_des_donn%C3%A9es_Off.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YJO76eqbwM",
        "outputId": "b3f61f21-327d-4b35-aed6-199455b47c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install bs4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W44FrfMl7pg",
        "outputId": "bce4a96e-d8f5-4d25-847b-509634246147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 245 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 286 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 288 kB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install dateparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvs8nNmejaYA"
      },
      "outputs": [],
      "source": [
        "#Importer les packages nécessaires\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "from pandas.core.frame import DataFrame\n",
        "import dateparser\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bBKZLKyAh3F"
      },
      "outputs": [],
      "source": [
        "# récupérer le code HTML sous format text\n",
        "def get_text(s):\n",
        "   url=\"https://www.lemonde.fr/recherche/?search_keywords=crise&start_at=08/01/2022&end_at=08/02/2022&search_sort=date_desc&page=\"+str(s)\n",
        "   rep = requests.get(url)\n",
        "   if rep.ok:\n",
        "     soup=BeautifulSoup(rep.text)\n",
        "   else :\n",
        "    print(\" Erreur Request get\")\n",
        "  # if rep not ok ==> rep.text = ?  \n",
        "   return rep.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7HoulJPCXLl"
      },
      "outputs": [],
      "source": [
        "#Récupérer temps de lecture, catégorie, date de publication\n",
        "def get_read_time_categorie(url2):\n",
        "    catégorie=''\n",
        "    date_de_publication=''\n",
        "    temps_de_lecture=''\n",
        "    rep = requests.get(url2)\n",
        "    soup2=BeautifulSoup(rep.text)\n",
        "    p=soup2.find('p',{'class':\"meta__reading-time\"})\n",
        "    s=soup2.find('span',{'class':\"meta__reading-time\"})\n",
        "    if p:\n",
        "      temps_de_lecture=(re.sub(\"temps de lecture[-_/./,]*\",\"\",p.get_text().lower()))\n",
        "    if s:\n",
        "      # if p and s,first temps_de_lecture will  be overridden \n",
        "      temps_de_lecture=(re.sub(\"temps de lecture[-_/./,]*\",\"\",s.get_text().lower()))\n",
        "    sections=soup2.findAll('section', {'class': True})\n",
        "    for section in sections:\n",
        "      if('zone' or 'article' or ('meta' and 'date') in section['class']):\n",
        "        aa=section.findAll('a',{'class': True})\n",
        "        span_date=section.findAll('span',{'class': 'meta__date'})\n",
        "        for a in aa :\n",
        "            if((\"logo\" or \"article\") in a['class'][0]):\n",
        "              catégorie=a.text\n",
        "              break\n",
        "        for span in span_date:\n",
        "           date_de_publication=dateparser.parse(re.sub(\"publié|le|à|[-_/./,]*|mis(e)* à* jour.*\",'',(span.text).lower()))\n",
        "      return catégorie,date_de_publication,temps_de_lecture\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Récupérer title2 depuis URL de l'article\n",
        "def get_title_2(url):\n",
        "  title2=''\n",
        " # p='-*[A-ZA-z0-9]+-[A-Za-z0-9]+'\n",
        "  pattern='(/article/)([0-9]+/)+(-*[A-ZA-z0-9]+-[A-Za-z0-9]+)+_'\n",
        "  m=re.search(pattern,url)\n",
        "  if m:\n",
        "    title2=re.sub('(/article/)([0-9]+/)+|-|_',' ',m.group())\n",
        "    # return re.sub('(/article/)([0-9]+/)+|-|_',' ',m.group()).strip() -- bla matzidi title2\n",
        "  return title2.strip()"
      ],
      "metadata": {
        "id": "XF-4Z6lTZQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7wVYxf96g0n"
      },
      "outputs": [],
      "source": [
        "#Récupérer title1, lien de l'article ainsi que les autres attributs et  les stocker dans une dataframe\n",
        "def get_all_line_dataframe(rep,df):\n",
        "  title=''\n",
        "  categorie=date_de_publication=read_time=''\n",
        "  soup=BeautifulSoup(rep)\n",
        "  sections=soup.findAll('section')\n",
        "  for section in sections:\n",
        "    if(section['class'][0]==\"teaser\"):\n",
        "        aa=section.findAll('a')\n",
        "        titles=section.findAll('h3')\n",
        "        for a in aa:\n",
        "          urla=a['href']\n",
        "          if '/article/'in a['href']:\n",
        "            categorie,date_de_publication,read_time=get_read_time_categorie(urla)\n",
        "            # else blach makadir walo\n",
        "          else :\n",
        "            continue\n",
        "        for title in titles:\n",
        "          title=title.text\n",
        "    if categorie.strip()=='Économie'and not (pd.Series([title]).isin(df.title)[0]):\n",
        "     title2=get_title_2(a['href'])\n",
        "     df=df.append(\n",
        "    {\"title\": title,\n",
        "     \"title2\":title2,\n",
        "     \"categorie\": categorie,\n",
        "     \"date\":date_de_publication,\n",
        "     \"read\":read_time,\n",
        "     \"lien\":a['href']\n",
        "     }, ignore_index=True)\n",
        "        # had condition zidiha m3a if lawla and w safi  \n",
        "     if(len(df)>=2):\n",
        "       break\n",
        "  return df      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSmYRSySt1wK"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  df = pd.DataFrame(columns=['title','categorie','date','read','lien','title2'])\n",
        "  s=1\n",
        "  # hadi diriha \n",
        "  # for index in range(len(df)):\n",
        "  #    df=get_all_line_dataframe(get_text(index),df)\n",
        "  while len(df)<5:\n",
        "    rep=get_text(s)\n",
        "    df=get_all_line_dataframe(rep,df)\n",
        "    s=s+1\n",
        "    sh=logging.debug(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Enregistrer dans un csv et créer dossier si not exist\n",
        "\n",
        "outname = 'myData.csv'\n",
        "\n",
        "outdir = r'C:\\file_csv'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "with open(fullname, 'a') as f:\n",
        "    df.to_csv(f, header=f.tell()==0,index=False)"
      ],
      "metadata": {
        "id": "1sxxKpxqUEmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "4fa470ac-7dd7-4e82-e829-608a4951491f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9f9ba73c6f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfullname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Scraping des données Off",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}